---
title: "Lab 08 - University of Edinburgh Art Collection"
author: "Fanyi Zeng"
date: "03/11/22"
output: github_document
---

### Load packages and data

```{r load-packages, message = FALSE}
library(robotstxt)
library(rvest)
library(tidyverse)
```

```{r load-data, message = FALSE, eval = FALSE}
paths_allowed("https://collections.ed.ac.uk/art)")
```

### Read the page

```{r url}
# set url
first_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0"
# read html page
page <- read_html(first_url)
```

### Save the titles

```{r titles}
titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()
titles
```

### Save the links

```{r links}
links <- page %>%
  html_nodes(".iteminfo") %>%   # same nodes
  html_node("h3 a") %>%         # as before
  html_attr("href") %>%         # but get href attribute instead of text
  str_replace(".", "https://collections.ed.ac.uk/art")
links
```

### Save the artists

```{r artists}
artists <- page %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_text()
artists
```

### Create a dataframe

```{r df1}
first_ten <- tibble(
  title = titles,
  artist = artists,
  link = links
) 
first_ten  
```

### Scrape the next page

```{r url2}
# set url
second_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=10"
# read html page
page2 <- read_html(second_url)
```

### Save the titles

```{r titles2}
titles2 <- page2 %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()
titles2
```

### Save the links

```{r links2}
links2 <- page2 %>%
  html_nodes(".iteminfo") %>%   # same nodes
  html_node("h3 a") %>%         # as before
  html_attr("href") %>%         # but get href attribute instead of text
  str_replace(".", "https://collections.ed.ac.uk/art")
links2
```

### Save the artists

```{r artists2}
artists2 <- page2 %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_text()
artists2
```

### Create a dataframe

```{r df1}
second_ten <- tibble(
  title = titles2,
  artist = artists2,
  link = links2
) 
second_ten  
```

### Create a function

```{r function}
scrape_page <- function(url){
   page <- read_html(url)
titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()
links <- page %>%
  html_nodes(".iteminfo") %>%   
  html_node("h3 a") %>%         
  html_attr("href") %>%         
  str_replace(".", "https://collections.ed.ac.uk/art")
artists <- page %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_text()
ten <- tibble(
  title = titles,
  artist = artists,
  link = links)
}
```

### Test the function

It works!!

```{r test}
first_page <- scrape_page(first_url)
second_page <- scrape_page(second_url)
```

### Iteration

```{r iterate}
paste0("https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=",seq(0,2900,by=10))

```
### Exercise 9

```{r separate-title-date, error = TRUE}
uoe_art <- uoe_art %>%
  separate(title, into = c("title", "date"), sep = "\\(") %>%
  mutate(year = str_remove(date, "\\)") %>% as.numeric()) %>%
  select(title, artist, year, ___)
```

### Exercise 10

Remove this text, and add your answer for Exercise 10 here.
Add code chunks as needed.
Don't forget to label your code chunk.
Do not use spaces in code chunk labels.

### Exercise 11

...

Add exercise headings as needed.
